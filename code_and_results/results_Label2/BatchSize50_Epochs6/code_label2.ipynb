{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "# Choix du label \n",
    "\n",
    "label = 'Label_2'\n",
    "nombre_labels = 5\n",
    "\n",
    "#label = 'Label_1'\n",
    "#nombre_labels = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing standard libraries for every machine/deep learning pipeline\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Importing specific libraries for data prerpcessing, model archtecture choice, training and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import CamembertTokenizer, CamembertForSequenceClassification\n",
    "from transformers import AdamW\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from numpy.random import seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x16571962190>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Seed value\n",
    "# Apparently you may use different seed values at each stage\n",
    "seed_value= 0\n",
    "\n",
    "# 1. Set the `PYTHONHASHSEED` environment variable at a fixed value\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED']=str(seed_value)\n",
    "\n",
    "# 2. Set the `python` built-in pseudo-random generator at a fixed value\n",
    "import random\n",
    "random.seed(seed_value)\n",
    "\n",
    "# 3. Set the `numpy` pseudo-random generator at a fixed value\n",
    "import numpy as np\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 4. Set the `tensorflow` pseudo-random generator at a fixed value\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(seed_value)\n",
    "\n",
    "# 5. Pytorch random number generator\n",
    "torch.manual_seed(seed_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining constants\n",
    "epochs = 6\n",
    "MAX_LEN = 128\n",
    "batch_size = 50\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "\n",
    "df_training = pd.read_csv('Projet_Tweet_vaccin_fichier_1.1_labelises_pour_creer_la_fonction.csv', delimiter = \";\", encoding = 'latin_1')\n",
    "df_validation = pd.read_csv('Projet_Tweet_vaccin_fichier_1.2_labelises_pour_tester_la_fonction.csv', delimiter = \";\", encoding = 'latin_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([df_training, df_validation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CamemBERT tokenizer\n",
    "tokenizer = CamembertTokenizer.from_pretrained('camembert/camembert-large',do_lower_case=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    }
   ],
   "source": [
    "# Creates list of texts and labels\n",
    "text = df['text'].to_list()\n",
    "labels = df[label].to_list()\n",
    "\n",
    "#user tokenizer to convert tokenizer = CamembertTokenizer.from_pretrained('camembert-base',do_lower_case=True)ces into tokenizer\n",
    "input_ids  = [tokenizer.encode(sent,add_special_tokens=True,max_length=MAX_LEN) for sent in text]\n",
    "\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "    seq_mask = [float(i>0) for i in seq]  \n",
    "    attention_masks.append(seq_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels, train_masks, validation_masks = train_test_split(input_ids, labels, attention_masks,\n",
    "                                                            random_state=42, test_size=0.1)\n",
    "\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = SequentialSampler(validation_data)\n",
    "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at camembert/camembert-large were not used when initializing CamembertForSequenceClassification: ['roberta.pooler.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
      "- This IS expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing CamembertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at camembert/camembert-large and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CamembertForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(32005, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (12): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (13): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (14): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (15): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (16): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (17): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (18): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (19): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (20): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (21): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (22): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (23): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=5, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load CamembertForSequenceClassification, the pretrained camembert model with a single linear classification layer on top. \n",
    "model = CamembertForSequenceClassification.from_pretrained('camembert/camembert-large', num_labels = nombre_labels)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\cdcha\\Anaconda3\\envs\\transformers8\\lib\\site-packages\\transformers\\optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = AdamW(optimizer_grouped_parameters, lr=2e-5, eps=10e-8)\n",
    "\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:   0%|          | 0/6 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 1.4489217047338132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  17%|█▋        | 1/6 [19:46<1:38:50, 1186.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.595072463768116\n",
      "Train loss: 1.1067720232186493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  33%|███▎      | 2/6 [39:28<1:18:54, 1183.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.518840579710145\n",
      "Train loss: 0.9275520576371087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████     | 3/6 [59:09<59:08, 1182.77s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6095652173913043\n",
      "Train loss: 0.633631357440242\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  67%|██████▋   | 4/6 [1:18:51<39:24, 1182.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6034782608695651\n",
      "Train loss: 0.4729824866409655\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  83%|████████▎ | 5/6 [1:38:32<19:41, 1181.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6246376811594203\n",
      "Train loss: 0.3275606566005283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 6/6 [1:58:14<00:00, 1182.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6713043478260868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Store our loss and accuracy for plotting if we want to visualize training evolution per epochs after the training process\n",
    "train_loss_set = []\n",
    "loss_train = []\n",
    "accuracy_validation = []\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "    # Tracking variables for training\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "  \n",
    "    # Train the model\n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # Add batch to device CPU or GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Clear out the gradients (by default they accumulate)\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        outputs = model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        # Get loss value\n",
    "        loss = outputs[0]\n",
    "        # Add it to train loss list\n",
    "        train_loss_set.append(loss.item())    \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        optimizer.step()\n",
    "    \n",
    "        # Update tracking variables\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    loss_train.append(tr_loss/nb_tr_steps)\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "    \n",
    "    \n",
    "    # Tracking variables for validation\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    # Validation of the model\n",
    "    model.eval()\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "        # Add batch to device CPU or GPU\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        # Unpack the inputs from our dataloader\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "        with torch.no_grad():\n",
    "            # Forward pass, calculate logit predictions\n",
    "            outputs =  model(b_input_ids,token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss, logits = outputs[:2]\n",
    "    \n",
    "        # Move logits and labels to CPU if GPU is used\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "    \n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    \n",
    "    accuracy_validation.append(eval_accuracy/nb_eval_steps)\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8/fFQqAAAACXBIWXMAAAsTAAALEwEAmpwYAAAnvUlEQVR4nO3deXhV5b328e8vM0kgDAlTAoEwyoyEMCmTlaK1xVkGRRAJqNUWbR36nr61r+2x9bRqrTiEQYoo4IRDrVhRBoUESBSUeQwQkBnCGEKS5/0jgUMxQIDsrOzs+3Nducjee2Xl3qLee631POsx5xwiIhK4grwOICIi3lIRiIgEOBWBiEiAUxGIiAQ4FYGISIAL8TrAxYqNjXVNmjTxOoaIiF/Jysra65yLK+01vyuCJk2akJmZ6XUMERG/YmZbzvWaTg2JiAQ4FYGISIBTEYiIBDi/u0YgIpXLyZMnycnJIS8vz+soAkRERJCQkEBoaGiZf0ZFICKXJScnh+rVq9OkSRPMzOs4Ac05x759+8jJyaFp06Zl/jmdGhKRy5KXl0edOnVUApWAmVGnTp2LPjpTEYjIZVMJVB6X8ncRMEWw78gJfv/RSvJOFnodRUSkUgmYIkjftI/XFmYz5vUslYFIFbJv3z46depEp06dqF+/PvHx8acf5+fnn/dnMzMzeeihhy74O3r27FkuWefNm8cNN9xQLvsqTwFzsfiGDg05klfAE7O+Y/TUTCYMTyYiNNjrWCJymerUqcOyZcsAePLJJ4mOjuZXv/rV6dcLCgoICSn9f3XJyckkJydf8HcsWrSoXLJWVgFzRAAwOKUxf765A19t2MvoqZkcz9eRgUhVNGLECB5++GH69evHY489xpIlS+jZsyedO3emZ8+erF27FvjPT+hPPvkk99xzD3379iUpKYkXXnjh9P6io6NPb9+3b19uvfVWWrduzbBhwzi1yuO//vUvWrduzVVXXcVDDz10UZ/8p0+fTvv27WnXrh2PPfYYAIWFhYwYMYJ27drRvn17nnvuOQBeeOEF2rRpQ4cOHRg8ePDl/8MigI4ITrm9ayPM4NF3v+XeqUuZOLwr1cJ0ZCBSHn7/0UpW7ThUrvts07AGv/tp24v+uXXr1jFnzhyCg4M5dOgQCxYsICQkhDlz5vCb3/yGd9999wc/s2bNGubOncvhw4dp1aoV99133w/G43/zzTesXLmShg0b0qtXLxYuXEhycjJjxoxhwYIFNG3alCFDhpQ5544dO3jsscfIysqiVq1aDBgwgPfff59GjRqxfft2VqxYAcDBgwcB+NOf/sTmzZsJDw8//dzlCqgjglNuS27EX27tyKKN+7hnylKO5Rd4HUlEytltt91GcHDxh7zc3Fxuu+022rVrx7hx41i5cmWpP/OTn/yE8PBwYmNjqVu3Lrt27frBNikpKSQkJBAUFESnTp3Izs5mzZo1JCUlnR67fzFFsHTpUvr27UtcXBwhISEMGzaMBQsWkJSUxKZNm3jwwQeZPXs2NWrUAKBDhw4MGzaMadOmnfOU18UKuCOCU27pkkBQEDzy1nLumbKUySO6EhkWsP84RMrFpXxy95WoqKjT3//2t7+lX79+zJo1i+zsbPr27Vvqz4SHh5/+Pjg4mIKCH35ILG2bU6eHLsW5frZWrVosX76cTz/9lPHjx/PWW28xefJkPv74YxYsWMCHH37IU089xcqVKy+7EHx2RGBmk81st5mtuMB2Xc2s0Mxu9VWWc7mpcwLP3dGJJZv3M+K1pRw9oSMDkaooNzeX+Ph4AKZMmVLu+2/dujWbNm0iOzsbgJkzZ5b5Z7t168b8+fPZu3cvhYWFTJ8+nT59+rB3716Kioq45ZZbeOqpp/j6668pKipi27Zt9OvXj2eeeYaDBw9y5MiRy87vy4/AU4AXgann2sDMgoE/A5/6MMd5DeoUj5kxbuYyRry2hNdGphAdriMDkark0Ucf5e677+bZZ5+lf//+5b7/atWq8dJLLzFw4EBiY2NJSUk557aff/45CQkJpx+//fbbPP300/Tr1w/nHNdffz2DBg1i+fLljBw5kqKiIgCefvppCgsLufPOO8nNzcU5x7hx46hZs+Zl57fLOaS54M7NmgD/dM61O8frvwROAl1LtnvnQvtMTk52vliY5p/f7uAXM5bRuVFNXhvZleoRZb9hk0ggW716NVdccYXXMTx35MgRoqOjcc7xwAMP0KJFC8aNG+dJltL+TswsyzlX6lhZzy4Wm1k8cBPwShm2TTWzTDPL3LNnj0/y3NChIX8f0pll2w5y9+QlHM476ZPfIyJV04QJE+jUqRNt27YlNzeXMWPGeB2pzLwcNfQ88Jhz7oKD+Z1zac65ZOdcclxcqUtulovr2zfgxaGd+TYnl+GTl3BIZSAiZTRu3DiWLVvGqlWreOONN4iMjPQ6Upl5WQTJwAwzywZuBV4ysxs9zAPAwHYNGD/sSr7LyeWuSUvIPa4yELkQX55ilotzKX8XnhWBc66pc66Jc64J8A5wv3Pufa/ynOnHbevz8p1dWLUjl7smLSb3mMpA5FwiIiLYt2+fyqASOLUeQURExEX9nM+Gx5jZdKAvEGtmOcDvgFAA59wFrwt47do29Xjlzi7cN+1r7py0mNdHpVAzMszrWCKVTkJCAjk5Ofjq+p1cnFMrlF0Mn44a8gVfjRo6ly/W7GLs61/Tol40b9zbTWUgIn6pUo4a8hf9W9fj1eFdWL/7CEMnLObA0fPf1lZExN+oCMqgX6u6TBiezIY9Rxg6cTH7VQYiUoWoCMqoT8s4Jt2dzKY9Rxg6IYN9R054HUlEpFyoCC7C1S3imDyiK9n7jjJ0wmL2qgxEpApQEVykXs1jmXx3V7bsP8qQtAz2HFYZiIh/UxFcgp7NY3ltRAo5B44zZEIGuw/neR1JROSSqQguUY9mdZgysis7Dh5nSFoGuw+pDETEP6kILkO3pDpMGZnC97l5DE7LYJfKQET8kIrgMqU0rc3Ue1LYdai4DHbmqgxExL+oCMpBcpPaTB2Vwp7DJxicls73uce9jiQiUmYqgnLSJbG4DPYdyeeOVzPYflBlICL+QUVQjq5sXIupo1I4cDSfwWnp5Bw45nUkEZELUhGUs86NazHt3m4cPHaSwWkZbNuvMhCRyk1F4AMdG9XkjXu7cei4ykBEKj8VgY90SKjJm6O7c+REAYPTMti6T2UgIpWTisCH2sXH8Ma93TiaX8Adaels2XfU60giIj+gIvCxdvExvHlvd/JOFnLHqxlk71UZiEjloiKoAG0a1uDN0d3JLyzijrR0Nu054nUkEZHTVAQV5IoGNZg+ujsFhY7BaRlsVBmISCWhIqhArepXZ3pqd4pccRls2K0yEBHvqQgqWMt61Zk+ujvOweC0DNbvOux1JBEJcCoCD7SoV50Zqd0xgyETMlinMhARD6kIPNK8bjQzUrsTZMaQtAzW7lQZiIg3VAQeahZXXAYhwcaQCRms/v6Q15FEJACpCDyWFBfNjNQehAUHMXRCBqt2qAxEpGKpCCqBprFRzBzTnWqhwQydmMGK7bleRxKRAOKzIjCzyWa228xWnOP1YWb2bcnXIjPr6Kss/iCxThQzUnsQFRbCsImLVQYiUmF8eUQwBRh4ntc3A32ccx2Ap4A0H2bxC43rRDIjtTvR4SEMnZDBdzkqAxHxPZ8VgXNuAbD/PK8vcs4dKHmYAST4Kos/aVS7uAxqVAtl6MQMlm876HUkEaniKss1glHAJ+d60cxSzSzTzDL37NlTgbG8caoMakaGcufExXyz9cCFf0hE5BJ5XgRm1o/iInjsXNs459Kcc8nOueS4uLiKC+ehhFqRzEjtQa2oMIZPWsLXKgMR8RFPi8DMOgATgUHOuX1eZqmM4mtWY+aY7tSOLi6DrC3nPNMmInLJPCsCM2sMvAfc5Zxb51WOyq5BTDVmpvYgrno4wyctITNbZSAi5cuXw0enA+lAKzPLMbNRZjbWzMaWbPJ/gTrAS2a2zMwyfZXF39WPiWBGanfq1Yhg+OQlLNmsMhCR8mPOOa8zXJTk5GSXmRmYnbH7UB6DJ2SwMzePySO60j2pjteRRMRPmFmWcy65tNc8v1gsZVe3RvGRQcOa1Rj52lLSN+qyiohcPhWBn6lbPYLpo7uTUKsaI6csYdGGvV5HEhE/pyLwQ3HVw5me2p3GtSMZOWUpX61XGYjIpVMR+KnY6HCmj+5O09goRv1jKV+ur/oT7UTEN1QEfqxOdDhvni6DTOavUxmIyMVTEfi52lFhTB/dneZx0Yyemsnctbu9jiQifkZFUAXUigrjzdHdaFE3mjFTs/hizS6vI4mIH1ERVBE1I8N4495utKpfnTGvZzFnlcpARMpGRVCF1IwMY9qobrRpUIP73sjiM5WBiJSBiqCKiYkMZeqobrRpGMP9b2Qxe8VOryOJSCWnIqiCYqqF8vqoFNo2jGHstCweePNrNuw+4nUsEamkVARVVI2IUN64txsP9m/OvDW7GfDcfB55aznb9h/zOpqIVDK66VwA2HfkBC/P28jUjC0457ijayMe7N+CejUivI4mIhXkfDedUxEEkJ25efz9i/XMXLqN4CBjeI9ExvZpRp3ocK+jiYiPqQjkP2zbf4zn56xn1jc5VAsN5p6rmnLv1UnEVAv1OpqI+IiKQEq1YfdhnvtsPR9/9z0x1UJJ7Z3EyF5NiAwL8TqaiJQzFYGc14rtuTz72Tq+WLOb2Ogw7u/bnKHdGhMRGux1NBEpJyoCKZOsLQf4y6drSd+0jwYxETx0TQtu7ZJAaLAGl4n4O61QJmXSJbEW01O78+a93agfE8ET733Hj56dz6xvcigs8q8PDCJSdioC+YGezWN5776eTLo7mciwEMbNXM51f1vA7BXf429HkCJyYSoCKZWZcc0V9fj4wat4cWhnCoocY6d9zc9eXMi8tbtVCCJViIpAzisoyLihQ0P+/cve/M+tHThwLJ8Rry3l9lfTWbxpn9fxRKQc6GKxXJT8giJmZm7jxS/Ws+vQCa5uEcuvBrSiY6OaXkcTkfPQqCEpd3knC3k9fQsvz9/I/qP5XNumHo8MaEnr+jW8jiYipVARiM8cOVHA5K82M2HBJo7kF/DTDg0Zd21LmsZGeR1NRM6gIhCfO3gsn7QFm3htYTb5hUXcemUCD17TnIRakV5HExE8mkdgZpPNbLeZrTjH62ZmL5jZBjP71syu9FUW8b2akWE8OrA1Cx7tx/Aeicz6Zjv9/zKf332wgt2H87yOJyLn4ctRQ1OAged5/TqgRclXKvCyD7NIBYmrHs7vftqWeb/uyy1d4pm2eCu9n5nL05+s5sDRfK/jiUgpfFYEzrkFwP7zbDIImOqKZQA1zayBr/JIxWpYsxpP39yBzx/uw8C29UlbsInez8zl+TnrOJx30ut4InIGL+cRxAPbznicU/LcD5hZqpllmlnmnj17KiSclI8msVE8P7gzn/6yN72ax/L8nPX0fmYur87fyPH8Qq/jiQjeFoGV8lypV66dc2nOuWTnXHJcXJyPY4kvtKxXnVfu6sJHP7+KDgk1efqTNfT+n7n8Y1E2JwpUCCJe8rIIcoBGZzxOAHZ4lEUqSPuEGP5xTwpvjelB09gofvfhSvr/ZT5vLd1GQWGR1/FEApKXRfAhMLxk9FB3INc5972HeaQCpTStzczU7ky9J4XY6DAeffdbBjy3gA+X76BIdzoVqVA+m0dgZtOBvkAssAv4HRAK4Jx7xcwMeJHikUXHgJHOuQtOENA8gqrHOcdnq3bx13+vY+2uw7SuX52Hr23JtW3qUfyviYhcLk0oE79QVOT46NsdPD9nPZv3HqVjo5r8akBLrmoeq0IQuUxamEb8QlCQMahTPJ+N682fb2nP3sMnuGvSEganZZCZfb6RyCJyOXREIJXWiYJCZizZxt+/2MDeIyfo2yqOR65tRfuEGK+jifgdnRoSv3Y8v5B/pGfzyvyNHDx2koFt6/PwgJa0rFfd62gifkNFIFXCobyTTPpyM5O+2szR/AJu7BTPL3/UgsQ6utOpyIWoCKRK2X80n1fnb+Qf6dkUFDpuS27EQ9c0p0FMNa+jiVRaKgKpknYfymP83A28uWQrZsad3RK5r28z4qqHex1NpNK57CIwsyjguHOuyMxaAq2BT5xzFX73MBWBnC3nwDFe+Hw972TlEB4SzNg+zfh5/+YEB2nIqcgp5TF8dAEQYWbxwOfASIpvMy3iuYRakTxza0fmPNyH/q3r8tycdYydlsWx/AKvo4n4hbIWgTnnjgE3A393zt0EtPFdLJGLlxQXzfhhV/LkT9vw+epd3PZKOjtztSiOyIWUuQjMrAcwDPi45LkQ30QSuTwjejVl0t1dyd57lEHjv2LF9lyvI4lUamUtgl8CTwCznHMrzSwJmOuzVCKXqV/rurxzX0+CzbjtlXQ+W7XL60gilVaZisA5N9859zPn3J/NLAjY65x7yMfZRC7LFQ1q8P4DvWhZL5rU1zOZ+OUm/G2UnEhFKFMRmNmbZlajZPTQKmCtmf3at9FELl/dGhHMSO3BwLb1+cPHq/mv91dwUuseiPyHsp4aauOcOwTcCPwLaAzc5atQIuWpWlgw44deydg+zXhj8VbumbKUQ1o3WeS0shZBqJmFUlwEH5TMH9AxtviNoCDj8eta88wtHUjfuI9bXlrEtv3HvI4lUimUtQheBbKBKGCBmSUCh3wVSsRXbu/aiKmjUth1KI8bxy8ka8sBryOJeK6sF4tfcM7FO+eud8W2AP18nE3EJ3o2i2XWA72IjghhyIQMPlqupbIlsJX1YnGMmT1rZpklX3+l+OhAxC81i4tm1v296JgQw4PTv+Hvn6/XiCIJWGU9NTQZOAzcXvJ1CHjNV6FEKkLtqDCm3duNmzrH89fP1vHIW8s5UVDodSyRClfW2cHNnHO3nPH492a2zAd5RCpUeEgwz97ekaaxUTz72TpyDhzn1bu6UCsqzOtoIhWmrEcEx83sqlMPzKwXcNw3kUQqlpnx0DUt+NvgTizLOchNLy1k454jXscSqTBlLYKxwHgzyzazbOBFYIzPUol4YFCneKaP7sbhvAJufmkR6Rv3eR1JpEKUddTQcudcR6AD0ME51xno79NkIh7oklib9x/oRVz1cO6atJi3Mrd5HUnE58p6RACAc+5QyQxjgId9kEfEc41qR/LufT3p0awOj77zLX+evYaiIo0okqrroorgLFr+SaqsmGqhTB7RlSEpjXl53kYeePNrjudrRJFUTZdTBPqIJFVaaHAQ/31TO/7rJ1cwe+VOBqels/uwFrqRque8RWBmh83sUClfh4GGF9q5mQ00s7VmtsHMHi/l9Rgz+8jMlpvZSjMbeRnvRaTcmRn3Xp3Eq3d2Yd2uI9w0fhFrduruKlK1nLcInHPVnXM1Svmq7pw77xwEMwsGxgPXUbys5RAzO3t5yweAVSUXovsCfzUzDeCWSmdA2/q8PbYHBUVF3PpyOnPX7vY6kki5uZxTQxeSAmxwzm1yzuUDM4BBZ23jgOpmZkA0sB/QiuNSKbWLj+GDB64isU4ko6YsZWp6tteRRMqFL4sgHjhz7F1OyXNnehG4AtgBfAf8wjmnVUOk0qofE8FbY3rQv3Vd/u8HK3nyw5UUakSR+DlfFkFpo4rO/i/mx8Ayiq83dAJeNLMaP9iRWeqpG97t2bOnvHOKXJSo8BBevSuZUVc1ZcqibEZPzeTICR3Iiv/yZRHkAI3OeJxA8Sf/M40E3iu5tfUGYDPQ+uwdOefSnHPJzrnkuLg4nwUWKavgIOO3N7ThDze2Y/66Pdz68iK2H9RdV8Q/+bIIlgItzKxpyQXgwcCHZ22zFbgGwMzqAa2ATT7MJFKu7uyeyGsjurL9wHFuHL+Q5dsOeh1J5KL5rAiccwXAz4FPgdXAW865lWY21szGlmz2FNDTzL4DPgcec87t9VUmEV/o3TKOd+/vSVhwEHekpTN7xfdeRxK5KOZvi3EkJye7zMxMr2OI/MCewydIfT2Tb7Ye5PHrWjOmdxLFA+JEvGdmWc655NJe8+WpIZGAElc9nOmju3NDhwb86ZM1PP7ud+QXaBCcVH5lXZhGRMogIjSYFwZ3pmlsFH//YgNb9x/jlTu7EBMZ6nU0kXPSEYFIOQsKMh4Z0Iq/3taRzC37uenlhWzZd9TrWCLnpCIQ8ZFbuiQwbVQ39h/N58bxC1mavd/rSCKlUhGI+FC3pDrMur8XtSLDGDZhMbO+yfE6ksgPqAhEfKxpbBTv3d+TKxNrMm7mcp79bB3+NlpPqjYVgUgFqBkZxtR7unFblwRe+Hw9v5ixjLyTWuhGKgeNGhKpIGEhQTxzaweaxkXxzOy15Bw4xoThydSJDvc6mgQ4HRGIVCAz4/6+zXlp2JWs3HGIG19ayPpdh72OJQFORSDigevbN2DmmB4czy/i5pcX8dV63VlFvKMiEPFIp0Y1ef+BnsTXrMbdry3hzcVbvY4kAUpFIOKhhFqRvD22B1c1j+U3s77jjx+v0kI3UuFUBCIeqx4RyqS7kxneI5EJX25m7LQsjuVroRupOCoCkUogJDiI/zeoHU/+tA2fr97F7a+mszM3z+tYEiBUBCKVyIheTZl4dzKb9xzlxvELWbE91+tIEgBUBCKVTP/W9Xh7bE/M4PZX05mzapfXkaSKUxGIVEJtGtbggwd60SwumtGvZzLxy026LYX4jIpApJKqWyOCmWO6M6BNPf7w8Wp++8EKCgq10I2UPxWBSCUWGRbCy8O6MKZPEtMytjJyylIO5Z30OpZUMSoCkUouKMh44ror+NPN7UnfuI9bXlrEtv3HvI4lVYiKQMRPDE5pzD/uSWHnoTxuemkhX2894HUkqSJUBCJ+pFfzWGbd34vIsBAGp2Xw0fIdXkeSKkBFIOJnmteN5v0HetEhPoYHp3/Di1+s14giuSwqAhE/VDsqjDdGd+PGTg35y7/Xkfp6Fit3aPKZXBotTCPip8JDgnnujk60ql+D8XM38NmqXVzdIpb7+jSjR7M6mJnXEcVPmL8dUiYnJ7vMzEyvY4hUKrnHT/LG4i1M/iqbvUdO0CEhhrF9mvHjtvUJDlIhCJhZlnMuudTXVAQiVUfeyULe/TqHCQs2kb3vGE1jo0jtncRNneOJCA32Op546HxF4NNrBGY20MzWmtkGM3v8HNv0NbNlZrbSzOb7Mo9IVRcRGsywbol8/khfxg+9kujwEJ547zuufmYuL8/bqMloUiqfHRGYWTCwDrgWyAGWAkOcc6vO2KYmsAgY6JzbamZ1nXO7z7dfHRGIlJ1zjkUb9/HK/I18uX4v1cNDGNq9MaN6NaVujQiv40kFOt8RgS8vFqcAG5xzm0pCzAAGAavO2GYo8J5zbivAhUpARC6OmdGreSy9mseyYnsur8zfyIQFm3jtq2xuvjKe1N5JJMVFex1TPObLIogHtp3xOAfodtY2LYFQM5sHVAf+5pybevaOzCwVSAVo3LixT8KKVHXt4mN4ceiVbNl3lLQFm3g7K4eZmdv4cZv6jO3bjE6NanodUTziyyIobajC2eehQoAuwDVANSDdzDKcc+v+44ecSwPSoPjUkA+yigSMxDpR/PGm9vzyRy2Zsmgzr6dvYfbKnXRPqs3YPs3o0zJOQ08DjC8vFucAjc54nACcPR8+B5jtnDvqnNsLLAA6+jCTiJSIqx7Or3/cmkVPXMP/uf4KsvceY8RrS7n+ha/4YNl23fI6gPiyCJYCLcysqZmFAYOBD8/a5gPgajMLMbNIik8drfZhJhE5S3R4CKN7J7Hg0X48c2sH8gsK+cWMZfT76zympmdzPL/Q64jiYz6dR2Bm1wPPA8HAZOfcH81sLIBz7pWSbX4NjASKgInOuefPt0+NGhLxraIix5zVu3hl/ka+3nqQ2lFhjOjZhOE9EqkZGeZ1PLlEmlAmIhfNOcfS7AO8PG8Dc9fuITIsmMFdG3Pv1U1pWLOa1/HkIqkIROSyrNl5iFfnb+LD5Tsw4GedGjK2TzNa1qvudTQpIxWBiJSLnAPHmPjlZmYu3cbxk4X86Iq6jOnTjK5NansdTS5ARSAi5Wr/0XympmczZVE2B4+dJDmxFmP7NKN/67oE6SZ3lZKKQER84lh+ATOXbmPil5vZfvA4LepGM6ZPM37WsSFhIVrupDJREYiIT50sLOKf3+7g1fmbWLPzMA1iIhh1VVOGpDQmKlzLnlQGKgIRqRDOOeat3cPL8zeyZPN+YqqFMrxHIiN6NqFOdLjX8QKaikBEKtzXWw/wyryN/HvVLsJDgrijayNGX51Eo9qRXkcLSCoCEfHMht2HSVuwiVnfbKfIwU/aN2BMnyTaNozxOlpAURGIiOd25uYx6atNvLl4K0fzC+ndMo6xfZLokaT1lSuCikBEKo3cYyeZtngLry3czN4j+XQsWV95gNZX9ikVgYhUOnknC3knK4e0BZvYuv8YSbFRjO6dxM1XxhMeovWVy5uKQEQqrcIixycrvueV+RtZsf0QcdXDuadXU4Z1b0yNiFCv41UZKgIRqfSccyzcsI+X529g4YZ9VA8PYVj3RO7p1UTrK5cDFYGI+JXvcorXV/5kxfeEBAVxS5d4Uns3o2lslNfR/JaKQET8Uvbeo6R9uYl3snI4WVjEwLb1GdunGR21vvJFUxGIiF/bfTiPKQuzeT1jC4fzCuiRVIfU3kn0aRmnm9yVkYpARKqEw3kneXPxViZ9tZndh0+QWCeSu7oncluXRsRE6sLy+agIRKRKyS8oYvbKnUxdlE3mlgNEhAZxU+d47urehDYNa3gdr1JSEYhIlbViey6vp2/h/WXbOVFQRNcmtRjeowkD29UnNFi3wj5FRSAiVd7BY/m8nZnD1Ixstu0/Tt3q4Qzt1pihKY01/BQVgYgEkMIix/x1u/nHoi3MX7eHkCBjYLv63N2zCcmJtQL2vkbnKwKtGCEiVUpwkNG/dT36t67H5r1HmZaxhbcyt/HPb7/nigY1uLtHIoM6xVMtTLexOEVHBCJS5R3LL+D9b3YwNT2bNTsPUyMihNuTG3FXj0QS6wTGJDWdGhIRofg2Fks272dqxhZmr9hJkXP0bRnH8J5N6NOias9JUBGIiJxlZ24eby7ZypuLt7L3SNWfk6AiEBE5h0CZk+BZEZjZQOBvQDAw0Tn3p3Ns1xXIAO5wzr1zvn2qCETEV86ek5DSpDbDeyby47b+PyfBkyIws2BgHXAtkAMsBYY451aVst1nQB4wWUUgIl6rinMSzlcEvqy4FGCDc26Tcy4fmAEMKmW7B4F3gd0+zCIiUmY1I8MY3TuJeb/qx+QRyVzRoAbPz1lPzz99wYPTv2Fp9n787bT6+fhyHkE8sO2MxzlAtzM3MLN44CagP9D1XDsys1QgFaBx48blHlREpDTnmpPw0fIdVWpOgi+PCEobh3V2hT4PPOacKzzfjpxzac65ZOdcclxcXHnlExEps6axUfz2hjYs/s01/PdN7XHO8fh739Htv+fwx49XsWXfUa8jXjJfHhHkAI3OeJwA7Dhrm2RgRsmU71jgejMrcM6978NcIiKXLDIshKHdGjMkpdHpOQmTF2Yz8avN9GtVl7t6JPrdnARfXiwOofhi8TXAdoovFg91zq08x/ZTgH/qYrGI+Juz5yQ0qRPJnZVsToInF4udcwXAz4FPgdXAW865lWY21szG+ur3iohUtPoxETx8bUsWPd6fF4Z0JjY6nD98vJruT3/OE+99x+rvD3kd8bw0oUxExAcq25wEzSwWEfFIaXMShnVLZEhKowqdk6AiEBHxWGnrJFzXvgF390ikSwWsk6D1CEREPHa+OQltGtTg7p6J/KyjN3MSdEQgIuKRs9dJiKkWyu3JCdzVvQmN60SW6+/SqSERkUrs9DoJ6VuYvbJ4nYR+reoyvEcivctpToKKQETET5xzTkJyI2KqXfqcBBWBiIifyS8o4pMV3zM1fQtZWw5QLTSYRwa05N6rky5pf7pYLCLiZ8JCghjUKZ5BneJPz0loWLOaT36XikBEpJJrFx/Dn2/t4LP9+/eSOyIictlUBCIiAU5FICIS4FQEIiIBTkUgIhLgVAQiIgFORSAiEuBUBCIiAc7vbjFhZnuALZf447HA3nKM4w/0ngOD3nNguJz3nOiciyvtBb8rgsthZpnnutdGVaX3HBj0ngODr96zTg2JiAQ4FYGISIALtCJI8zqAB/SeA4Pec2DwyXsOqGsEIiLyQ4F2RCAiImdREYiIBLiAKQIzG2hma81sg5k97nUeXzOzyWa228xWeJ2lophZIzOba2arzWylmf3C60y+ZmYRZrbEzJaXvOffe52pIphZsJl9Y2b/9DpLRTCzbDP7zsyWmVm5r9UbENcIzCwYWAdcC+QAS4EhzrlVngbzITPrDRwBpjrn2nmdpyKYWQOggXPuazOrDmQBN1bxv2cDopxzR8wsFPgK+IVzLsPjaD5lZg8DyUAN59wNXufxNTPLBpKdcz6ZQBcoRwQpwAbn3CbnXD4wAxjkcSafcs4tAPZ7naMiOee+d859XfL9YWA1EO9tKt9yxY6UPAwt+arSn+7MLAH4CTDR6yxVRaAUQTyw7YzHOVTx/0EEOjNrAnQGFnscxedKTpMsA3YDnznnqvp7fh54FCjyOEdFcsC/zSzLzFLLe+eBUgRWynNV+lNTIDOzaOBd4JfOuUNe5/E151yhc64TkACkmFmVPRVoZjcAu51zWV5nqWC9nHNXAtcBD5Sc+i03gVIEOUCjMx4nADs8yiI+VHKe/F3gDefce17nqUjOuYPAPGCgt0l8qhfws5Jz5jOA/mY2zdtIvuec21Hy525gFsWnu8tNoBTBUqCFmTU1szBgMPChx5mknJVcOJ0ErHbOPet1nopgZnFmVrPk+2rAj4A1nobyIefcE865BOdcE4r/O/7COXenx7F8ysyiSgY/YGZRwACgXEcDBkQROOcKgJ8Dn1J8AfEt59xKb1P5lplNB9KBVmaWY2ajvM5UAXoBd1H8KXFZydf1XofysQbAXDP7luIPPJ855wJiSGUAqQd8ZWbLgSXAx8652eX5CwJi+KiIiJxbQBwRiIjIuakIREQCnIpARCTAqQhERAKcikBEJMCpCERKmFnhGcNOl5XnXWrNrEkg3QlW/EuI1wFEKpHjJbdqEAkoOiIQuYCSe8H/ueS+/0vMrHnJ84lm9rmZfVvyZ+OS5+uZ2aySNQKWm1nPkl0Fm9mEknUD/l0yExgze8jMVpXsZ4ZHb1MCmIpA5H9VO+vU0B1nvHbIOZcCvEjx3S8p+X6qc64D8AbwQsnzLwDznXMdgSuBU7PYWwDjnXNtgYPALSXPPw50LtnPWN+8NZFz08xikRJmdsQ5F13K89lAf+fcppKb2u10ztUxs70UL4RzsuT5751zsWa2B0hwzp04Yx9NKL79Q4uSx48Boc65P5jZbIoXEXofeP+M9QVEKoSOCETKxp3j+3NtU5oTZ3xfyP9eo/sJMB7oAmSZma7dSYVSEYiUzR1n/Jle8v0iiu+ACTCM4mUiAT4H7oPTi8bUONdOzSwIaOScm0vxYis1gR8clYj4kj55iPyvaiUrfZ0y2zl3aghpuJktpvjD05CS5x4CJpvZr4E9wMiS538BpJXc8bWQ4lL4/hy/MxiYZmYxFC+g9FzJugIiFUbXCEQuwNcLh4t4TaeGREQCnI4IREQCnI4IREQCnIpARCTAqQhERAKcikBEJMCpCEREAtz/B5cpp1yKL55mAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(loss_train, label = 'Training Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Training_loss.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(accuracy_validation, label = 'Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig('Validation_accuracy.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_to_proba(x):\n",
    "\n",
    "    x = 1/(1+np.exp(-logits))\n",
    "\n",
    "    for i in range(x.shape[0]):\n",
    "        x[i,:] = x[i,:]/np.sum(x[i,:])\n",
    "    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = pd.read_csv('Projet_Tweet_vaccin_fichier_3_pour_tester_la_fonction.csv', delimiter = \";\", \n",
    "                             encoding = 'latin_1')['text'].to_list()\n",
    "comments = tt[0:3000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the comments\n",
    "tokenized_comments_ids = [tokenizer.encode(comment,add_special_tokens=True,max_length=MAX_LEN) for comment in comments]\n",
    "# Pad the resulted encoded comments\n",
    "tokenized_comments_ids = pad_sequences(tokenized_comments_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "\n",
    "# Create attention masks \n",
    "attention_masks = []\n",
    "for seq in tokenized_comments_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "\n",
    "prediction_inputs = torch.tensor(tokenized_comments_ids)\n",
    "prediction_masks = torch.tensor(attention_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the finetuned model (Camembert)\n",
    "flat_pred = []\n",
    "with torch.no_grad():\n",
    "    # Forward pass, calculate logit predictions\n",
    "    outputs =  model(prediction_inputs.to(device),token_type_ids=None, attention_mask=prediction_masks.to(device))\n",
    "    logits = outputs[0]\n",
    "    logits = logits.detach().cpu().numpy() \n",
    "    flat_pred.extend(np.argmax(logits, axis=1).flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data={\"tweet\": comments, \"classification\": flat_pred})\n",
    "df.to_csv(\"classification_label2.csv\",index=False, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
